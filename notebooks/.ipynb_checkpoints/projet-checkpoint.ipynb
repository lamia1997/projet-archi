{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f25bc38f-b97e-42be-ba90-1a1dd5693f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .config(\"spark.driver.memory\", \"5g\") \\\n",
    "    .appName('app') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94ebe73-8996-4f62-8419-11ce35e25390",
   "metadata": {},
   "source": [
    "### On installe nltk pour les stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4348c0d7-a502-464a-b8ee-de393e81ed56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk) (4.62.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b54fc4-a191-45cb-88e3-76a992fd8649",
   "metadata": {},
   "source": [
    "### On importe les bibliotheques que l'on va utiliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "91cd6982-a658-4634-9c3d-69874ab010eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import *\n",
    "\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import *\n",
    "from pyspark.ml.feature import  StringIndexer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "043c98a6-4dd7-4efe-9a57-c1fbadb30948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6e9022f8-d201-4fbb-bfea-9ea44cf30212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.corpus \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "85aeb781-1d82-4987-887a-6843c5aa9cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2aae29c9-a047-479d-81da-1c62e5fe6502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb360a3-c53c-43c5-98af-b9f9348aaf9f",
   "metadata": {},
   "source": [
    "### On convertit le csv en spark df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3275cd34-9f45-4283-98ec-bf5afb522d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"DBPEDIA_train.csv\"\n",
    "df_train = spark.read.format('csv').options(header=True,delimiter=',').load(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1b3b3388-d277-4a33-9bfe-e3b44af8144b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(text,StringType,true),StructField(l1,StringType,true),StructField(l2,StringType,true),StructField(l3,StringType,true)))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75980f6-5958-4db0-b52e-c7149d8a0a64",
   "metadata": {},
   "source": [
    "### On observe nos données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2dfd4b2e-6bd9-4067-8f46-81f843db801a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "240942"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e30af12-2298-46b4-8855-8a18dd9e91b2",
   "metadata": {},
   "source": [
    "## On remarque que nos données contiennent des lignes qui n'ont pas de labels donc il faut les degager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9caa5c0b-dc2b-4c78-8869-34144d981104",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.dropna()\n",
    "df_train=df_train.filter((df_train.l1 == \"Agent\") | (df_train.l1=='Event') | (df_train.l1=='Species') | (df_train.l1=='Place')| (df_train.l1=='UnitOfWork')| (df_train.l1=='SportsSeason')| (df_train.l1=='TopicalConcept'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ea213c14-b4ed-4df7-8c51-131a26ace728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:====================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|            l1|\n",
      "+--------------+\n",
      "|       Species|\n",
      "|  SportsSeason|\n",
      "|         Place|\n",
      "|    UnitOfWork|\n",
      "|         Event|\n",
      "|TopicalConcept|\n",
      "|         Agent|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_train.select('l1').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "800fa002-43dd-40eb-b93a-fc4d09c67220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "192235"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.select('l1').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4342b20-30eb-41b9-92b8-6402b7a9a515",
   "metadata": {},
   "source": [
    " On a finalement 7 labels et on a perdu environ 60k lignes sur 260k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9a7b42b4-d492-4059-9cce-a3663ed988a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_stopwords= stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163a3d00-c3af-4278-a24b-07960b986e14",
   "metadata": {},
   "source": [
    "##### On definit la liste des stop words que l'on veut enlever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4ce42f66-919c-4208-8d28-5de772c7bce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"]\n",
    "for i in l:\n",
    "    add_stopwords.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f05ac738-53d7-4168-a492-c5dd1afdcf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "36636ddc-009f-456a-9d74-f6fe9b533246",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in s:\n",
    "    add_stopwords.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "168f846b-434d-40dc-90e8-64684cb13f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let s go\n"
     ]
    }
   ],
   "source": [
    "print('let s go')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f7557c-a9e2-4224-913e-78fee5dc4139",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd9eb27-8009-4cc7-a6f3-5756146f2e3c",
   "metadata": {},
   "source": [
    "Pour cette etape on a voulu tester plusieurs types de pre-pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "408a7cfa-87e9-4666-8776-d9609383621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pipeline(level,pipeline,df):\n",
    "    label_stringIdx = StringIndexer(inputCol = \"l\"+str(level), outputCol = \"label\")\n",
    "    pipelineFit=pipeline.fit(df)\n",
    "    temp_df=pipelineFit.transform(df)\n",
    "    d=label_stringIdx.fit(temp_df)\n",
    "    new_df=d.transform(temp_df)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cc6747-5926-499e-a265-66674784b7f6",
   "metadata": {},
   "source": [
    "###### Count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "94dd8706-ee01-4cd2-a91f-f33832eac86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# stop words\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "634315ee-5d9c-4363-8333-6a41c0dc0de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.Pipeline"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ca9a8-1368-4e6b-834d-f9e8aea4d549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "33fccb3b-ca08-47c7-b26b-f6f41e75b017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset_l1_p1=apply_pipeline(1,pipeline,df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c44720f2-1aa4-4949-b3fb-03ddf47a3c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset_l2_p1=apply_pipeline(2,pipeline,df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7caf11eb-1b0e-44ed-8796-1eb7a899abad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset_l3_p1=apply_pipeline(3,pipeline,df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "124e5762-e62e-499d-85be-2b57ceb8678c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "print('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4105e3-3973-4441-b449-bddd457efe2c",
   "metadata": {},
   "source": [
    "Nos données sont pretes à etre utilisés dans des modeles de Ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02641f40-fbc3-49e5-bc8b-a09d903a5eb9",
   "metadata": {},
   "source": [
    "###### TF/IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e8a291a9-2d50-49db-ade9-b272910b93ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram=NGram(inputCol=stopwordsRemover.getOutputCol(),outputCol=\"Ngrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cfdc763b-05a0-4d9d-a602-b9cbd09741c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF=HashingTF(inputCol=ngram.getOutputCol(),outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bd9e47a3-89c4-4d0d-9112-a67cf71b1ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf=IDF(minDocFreq=3,inputCol=hashingTF.getOutputCol(),outputCol=\"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "936d28a4-48b0-4cb2-b7fa-21636ea2d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_prepro=Pipeline(stages=[regexTokenizer,stopwordsRemover,ngram,hashingTF,idf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c930eb96-d562-4561-9039-792d0268b06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset_l1_p2=apply_pipeline(1,pipeline_prepro,df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "65caeeed-59fc-4616-9ab2-3b43d7fd8c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset_l2_p2=apply_pipeline(2,pipeline_prepro,df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1b826f3b-312e-4989-b610-a576171fa19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset_l3_p2=apply_pipeline(3,pipeline_prepro,df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6716ed-bac2-4942-b09f-d4742e4079b1",
   "metadata": {},
   "source": [
    "Word2Vec prend trop de temps du coup on ne va pas le faire "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e41b01-952d-4cc2-aa9a-dac9e706be70",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48df404-a0bf-44b0-9c1c-bd71000622f4",
   "metadata": {},
   "source": [
    "##### definition de modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d8395c8a-7cd3-45d5-ab1e-69ac24e0c4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10, regParam=0.3,featuresCol='features',labelCol='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ab93d7-81de-44c3-ab57-ef915a650a80",
   "metadata": {},
   "source": [
    "###### level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9665b7a8-c1e7-467d-a0b7-2691a74c2dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model,dataset):\n",
    "    (trainingData, testData) = dataset.randomSplit([0.9, 0.1], seed = 100)\n",
    "    lrModel = lr.fit(trainingData)\n",
    "    return lrModel  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6da6d225-8506-4da2-b34a-44d200115fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fitl1_1=fit_model(lr,dataset_l1_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2df1a824-25c1-450f-a01a-5a596aa16758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.LogisticRegressionModel"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fitl1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5f53a5f7-9f6e-4bc8-8457-14697b2f12c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a4d4188e-ed36-4e87-a834-d205607c21b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitl1_1.write().overwrite().save(\"model_l1_p4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ff9b46ad-ec1a-4fc4-a03e-4cc545bf1bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=(\"model_l1_p3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4a9633bb-a7ab-4192-97b7-cc07fd09ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=LogisticRegressionModel.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4caba995-51a3-4af6-8519-53e01c61cff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.LogisticRegressionModel"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b81c7592-100b-41bb-95e7-ae0088c804f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=(\"model_l1_p2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1490e062-3f66-4b4f-b279-0227400befbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "m2=LogisticRegressionModel.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d8cc27b-2165-4fc8-b6db-f6a18febf0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.LogisticRegressionModel"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "946b4278-032c-46c3-ac55-d9d6eb6ff745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fitl1_2=fit_model(lr,dataset_l1_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "549a8ccf-4e16-4db1-b2a3-8c90394e7cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fitl1_2.write().overwrite().save(\"model_l1_p2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275d884c-b973-467d-9bcb-570df51ffbed",
   "metadata": {},
   "source": [
    "##### level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3a04da6b-ad37-4530-b07b-251bd57fdfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fitl2_1=fit_model(lr,dataset_l2_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e071a657-b5ed-45c1-8b92-56ebe82caf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/01 11:17:25 ERROR Executor: Exception in task 6.0 in stage 80.0 (TID 354)]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.linalg.DenseMatrix$.zeros(Matrices.scala:491)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:160)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.$anonfun$calculate$1(RDDLossFunction.scala:59)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction$$Lambda$4469/0x00000008417c3840.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat org.apache.spark.InterruptibleIterator.foldLeft(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat org.apache.spark.InterruptibleIterator.aggregate(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$4140/0x00000008415c4040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$4141/0x00000008415c6040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2984/0x00000008412bb840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "22/04/01 11:17:25 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 6.0 in stage 80.0 (TID 354),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.linalg.DenseMatrix$.zeros(Matrices.scala:491)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:160)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.$anonfun$calculate$1(RDDLossFunction.scala:59)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction$$Lambda$4469/0x00000008417c3840.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat org.apache.spark.InterruptibleIterator.foldLeft(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat org.apache.spark.InterruptibleIterator.aggregate(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$4140/0x00000008415c4040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$4141/0x00000008415c6040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2984/0x00000008412bb840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "22/04/01 11:17:26 ERROR TaskSetManager: Task 6 in stage 80.0 failed 1 times; aborting job\n",
      "22/04/01 11:17:26 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 80.0 failed 1 times, most recent failure: Lost task 6.0 in stage 80.0 (TID 354) (pyspark executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.linalg.DenseMatrix$.zeros(Matrices.scala:491)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:160)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.$anonfun$calculate$1(RDDLossFunction.scala:59)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction$$Lambda$4469/0x00000008417c3840.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat org.apache.spark.InterruptibleIterator.foldLeft(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat org.apache.spark.InterruptibleIterator.aggregate(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$4140/0x00000008415c4040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$4141/0x00000008415c6040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2984/0x00000008412bb840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2309)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1183)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1177)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:61)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\n",
      "\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:24)\n",
      "\tat breeze.optimize.LineSearch$$anon$1.calculate(LineSearch.scala:52)\n",
      "\tat breeze.optimize.LineSearch$$anon$1.calculate(LineSearch.scala:31)\n",
      "\tat breeze.optimize.StrongWolfeLineSearch.phi$1(StrongWolfe.scala:76)\n",
      "\tat breeze.optimize.StrongWolfeLineSearch.$anonfun$minimizeWithBound$7(StrongWolfe.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat breeze.optimize.StrongWolfeLineSearch.minimizeWithBound(StrongWolfe.scala:151)\n",
      "\tat breeze.optimize.StrongWolfeLineSearch.minimize(StrongWolfe.scala:62)\n",
      "\tat breeze.optimize.LBFGS.determineStepSize(LBFGS.scala:82)\n",
      "\tat breeze.optimize.LBFGS.determineStepSize(LBFGS.scala:38)\n",
      "\tat breeze.optimize.FirstOrderMinimizer.$anonfun$infiniteIterations$1(FirstOrderMinimizer.scala:60)\n",
      "\tat scala.collection.Iterator$$anon$7.next(Iterator.scala:140)\n",
      "\tat breeze.util.IteratorImplicits$RichIterator$$anon$2.next(Implicits.scala:74)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.trainImpl(LogisticRegression.scala:1009)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:628)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:495)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:286)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.linalg.DenseMatrix$.zeros(Matrices.scala:491)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:160)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.$anonfun$calculate$1(RDDLossFunction.scala:59)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction$$Lambda$4469/0x00000008417c3840.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat org.apache.spark.InterruptibleIterator.foldLeft(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat org.apache.spark.InterruptibleIterator.aggregate(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$4140/0x00000008415c4040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$4141/0x00000008415c6040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2984/0x00000008412bb840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\n",
      "22/04/01 11:17:27 ERROR Utils: Uncaught exception in thread Executor task launch worker for task 4.0 in stage 80.0 (TID 352)\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.scheduler.Task.$anonfun$run$2(Task.scala:152)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1442)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:150)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_52/2652894595.py\", line 1, in <module>\n",
      "    fitl2_2=fit_model(lr,dataset_l2_p2)\n",
      "  File \"/tmp/ipykernel_52/4087527417.py\", line 3, in fit_model\n",
      "    lrModel = lr.fit(trainingData)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 161, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 335, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 332, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\", line 1309, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JJavaError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/clientserver.py\", line 480, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/clientserver.py\", line 503, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_52/2652894595.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfitl2_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_l2_p2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_52/4087527417.py\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model, dataset)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestData\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlrModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlrModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2067\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2069\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2070\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_pdb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m                         \u001b[0;31m# drop into debugger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/ipykernel/zmqshell.py\u001b[0m in \u001b[0;36m_showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;34m'traceback'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;34m'ename'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0;34m'evalue'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         }\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0mgateway_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_return_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;31m# Note: technically this should return a bytestring 'str' rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1034\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \"\"\"\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36m_create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             self.gateway_property, self)\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_to_java_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_thread_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36mconnect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m                 self.socket = self.ssl_context.wrap_socket(\n\u001b[1;32m    401\u001b[0m                     self.socket, server_hostname=self.java_address)\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "fitl2_2=fit_model(lr,dataset_l2_p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d951973-566c-4013-8d18-f0b14ae44e80",
   "metadata": {},
   "source": [
    "##### level 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff0c6c-b7dd-4532-bece-9ec607b9b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitl3_1=fit_model(lr,dataset_l3_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd202a1d-5f58-45d2-846c-486425f15bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitl3_2=fit_model(lr,dataset_l3_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2b7d844d-ddc9-4800-a815-7b88fcd74f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=risque.select(col(\"label\"),col(\"text_class\")).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f80fcc1c-77e1-4f25-ba54-f54807f23d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "75f0dc61-cd1a-4027-8538-9e1e82a1138b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Species'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b069fe3c-d87d-4a83-8e96-4991f0685950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "n=df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "04888a46-77a8-4a53-bfc5-d74261279732",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(n[0])\n",
    "dico={}\n",
    "for row in range(0,len(n)):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77811803-f890-4384-a9f0-d54085cb1c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c138bef6-86ec-46f0-8ce0-1055380aea54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 156:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|                text|            l1|               words|            filtered|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|\"\"\"Echourouk TV\\\"...|         Agent|[echourouk, tv, a...|[echourouk, tv, a...|(10000,[1,2,3,4,5...|  0.0|[2.99682356653826...|[0.71343850416859...|       0.0|\n",
      "|\"3757 Anagolay, p...|         Place|[3757, anagolay, ...|[3757, anagolay, ...|(10000,[0,1,2,3,4...|  1.0|[0.45869983148295...|[0.00144256628818...|       1.0|\n",
      "|\"Abies hidalgensi...|       Species|[abies, hidalgens...|[abies, hidalgens...|(10000,[0,1,2,3,4...|  2.0|[-0.2417903089158...|[0.00138855432011...|       2.0|\n",
      "|\"American Communi...|    UnitOfWork|[american, commun...|[american, commun...|(10000,[0,1,2,3,4...|  5.0|[1.31072735681431...|[0.19633681796646...|       5.0|\n",
      "|\"Andrew \"\"Andy\\\"\"...|         Agent|[andrew, andy, ho...|[andrew, andy, ho...|(10000,[2,3,5,7,1...|  0.0|[3.51632495731472...|[0.83466838905424...|       0.0|\n",
      "|\"Antonia de Ovied...|         Agent|[antonia, de, ovi...|[antonia, de, ovi...|(10000,[0,1,2,3,5...|  0.0|[4.94780231464488...|[0.96868152572600...|       0.0|\n",
      "|\"Basic Education ...|         Agent|[basic, education...|[basic, education...|(10000,[0,1,3,4,6...|  0.0|[4.65994711716818...|[0.94968823507906...|       0.0|\n",
      "|\"Bergün/Bravuogn ...|         Place|[berg, n, bravuog...|[berg, n, bravuog...|(10000,[0,1,3,4,6...|  1.0|[1.62287574706747...|[0.11585685180258...|       1.0|\n",
      "|\"Candombe is an U...|TopicalConcept|[candombe, is, an...|[candombe, is, an...|(10000,[0,1,2,3,4...|  6.0|[1.43812813136211...|[0.32388999713883...|       0.0|\n",
      "|\"Charales is an o...|       Species|[charales, is, an...|[charales, is, an...|(10000,[0,1,4,9,1...|  2.0|[1.85608250771087...|[0.29320883644769...|       2.0|\n",
      "|\"Charleston Area ...|         Place|[charleston, area...|[charleston, area...|(10000,[0,1,2,3,4...|  1.0|[0.52464336149042...|[2.90757651766736...|       1.0|\n",
      "|\"Chris Murphy (bo...|         Agent|[chris, murphy, b...|[chris, murphy, b...|(10000,[0,1,2,3,4...|  0.0|[4.28681202451743...|[0.91938204766349...|       0.0|\n",
      "|\"Cytherea (born S...|         Agent|[cytherea, born, ...|[cytherea, born, ...|(10000,[0,2,4,6,8...|  0.0|[3.79843226227284...|[0.88084894190065...|       0.0|\n",
      "|\"Don Martin (May ...|         Agent|[don, martin, may...|[don, martin, may...|(10000,[0,2,5,6,9...|  0.0|[3.81198974095505...|[0.88883258025523...|       0.0|\n",
      "|\"Doug Suisman (bo...|         Agent|[doug, suisman, b...|[doug, suisman, b...|(10000,[0,1,2,3,4...|  0.0|[6.31317771484076...|[0.99351074788954...|       0.0|\n",
      "|\"Emanuil Gojdu Na...|         Agent|[emanuil, gojdu, ...|[emanuil, gojdu, ...|(10000,[0,3,4,10,...|  0.0|[2.56056075012448...|[0.52151192836067...|       0.0|\n",
      "|\"Erjon Bogdani (b...|         Agent|[erjon, bogdani, ...|[erjon, bogdani, ...|(10000,[0,2,3,4,7...|  0.0|[4.4780858260978,...|[0.94682016391495...|       0.0|\n",
      "|\"Ernest \"\"Ern\\\"\" ...|         Agent|[ernest, ern, wet...|[ernest, ern, wet...|(10000,[1,2,3,5,7...|  0.0|[3.74472615375287...|[0.87197515276490...|       0.0|\n",
      "|\"Fox Sports News ...|         Agent|[fox, sports, new...|[fox, sports, new...|(10000,[0,1,2,3,4...|  0.0|[5.74211511398744...|[0.98284905192475...|       0.0|\n",
      "|\"Frank Horace Hah...|         Agent|[frank, horace, h...|[frank, horace, h...|(10000,[0,1,2,3,4...|  0.0|[3.53256043750254...|[0.84353969415943...|       0.0|\n",
      "+--------------------+--------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d3a848f2-6385-4724-8756-df8b40877eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"label\",metricName='f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ba999-6d40-4d03-b0f6-38ba644ca209",
   "metadata": {},
   "source": [
    "###### Score avec Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1feb2169-dbf4-421b-92d1-585199c21a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9277851392097687"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cb56af-ca20-4bf4-9d6d-1ee6127d32d9",
   "metadata": {},
   "source": [
    "###### Score avec TF/IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d96b5e77-8861-4a61-9c63-da8d873aed5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.916509190912641"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6687e7-00fb-4550-805d-df99a66390f3",
   "metadata": {},
   "source": [
    "###### Score avec Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f25aff2-0694-4dee-bdc8-8a564e523585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8505022768303262"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(test3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc5b823-6688-4703-b43e-dfa13f98ebb1",
   "metadata": {},
   "source": [
    "### On passe à l'optimisation d'hyper parametres, tentative 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bd5827-627b-45dc-9812-f08c3190ff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"DBPEDIA_val.csv/DBPEDIA_val.csv\"\n",
    "df_val = spark.read.format('csv').options(header=True,delimiter=',').load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badfda57-7a0c-4f39-827e-dc9f885b35e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"label\",metricName='f1')\n",
    "grid = ParamGridBuilder().addGrid(lr.maxIter, [0, 1]).build()\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator,\n",
    "    parallelism=2)\n",
    "cvModel = cv.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9be0396-4255-48cf-ab65-a40697fbbd19",
   "metadata": {},
   "source": [
    "### Tentative de streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee53f1-752b-43a7-bb4e-0666f63082a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4393b1aa-3aff-46dd-9325-ffe85d1ee947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7626058a-4c25-4c38-9a2c-92ef0e9c5910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bfba60-1b69-4973-9124-a11551622b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7794dc3e-0857-4900-8fe7-354af68f807e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
